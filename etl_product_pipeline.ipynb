{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0448f2",
   "metadata": {},
   "source": [
    "## ETL Product Data Pipeline\n",
    "In this notebook, we will build a robust ETL pipeline to process product data from an API. The pipeline will:\n",
    "* Extract product data from API endpoint and return combined data from all pages as a pandas DataFrame.\n",
    "* Transform the data to standardize column names, handle nested, duplicated or missing values, and calculate additional metric like discounted price.\n",
    "* Load the cleaned data into a PostgreSQL database using a staging-to-main table pattern.\n",
    "* Log all steps and errors for traceability and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a4d66",
   "metadata": {},
   "source": [
    "### 1. Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653f4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7400a",
   "metadata": {},
   "source": [
    "### 2. Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5339cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=\"etl_product_pipeline.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    \n",
    ")\n",
    "logging.info(\"ETL pipeline started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb76d9",
   "metadata": {},
   "source": [
    "## 3. Defining Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c631bd8",
   "metadata": {},
   "source": [
    "### 3.1 Extract Function\n",
    "\n",
    "* Fetches paginated data from REST API with configurable limits\n",
    "* Extracts `products` key if exists, else uses full JSON response\n",
    "* Stops early when total record count reached\n",
    "* Skips empty pages, continues fetching\n",
    "* Combines into single raw DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20719f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(base_url, key=\"products\", limit=30, max_pages=20):\n",
    "    all_data = []\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        for page in range(max_pages):\n",
    "            url = f\"{base_url}?limit={limit}&skip={page * limit}\"\n",
    "            try:\n",
    "                response = session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                # Extract relevant data\n",
    "                page_data = data.get(key, data)\n",
    "                if not page_data:\n",
    "                    logging.debug(f\"Empty page {page}, continuing\")\n",
    "                    continue\n",
    "                \n",
    "                all_data.extend(page_data) \n",
    "                logging.info(f\"Page {page}: {len(page_data)} records\")\n",
    "                \n",
    "                # Stop if we've fetched all records\n",
    "                if (total := data.get(\"total\")) and len(all_data) >= total:\n",
    "                    logging.info(f\"All {total} records fetched\")\n",
    "                    break\n",
    "                    \n",
    "            except requests.RequestException as e:\n",
    "                logging.error(f\"Error on page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    logging.info(f\"Extract completed: {len(df)} records fetched\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd1deb",
   "metadata": {},
   "source": [
    "### 3.2 Transform Function\n",
    "* Renames `discountPercentage` to `discount_percentage`.\n",
    "* Explodes `reviews` into rows and extracts `reviewer_rating` and `reviewer_name`.\n",
    "* Drops rows with missing critical fields and duplicate reviews.\n",
    "* Validates `price` and `discount_percentage`, calculates `price_with_discount`.\n",
    "* Converts column types and reorders for consistency.\n",
    "* Returns a clean, standardized DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541cb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    # Copy DF to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Rename the column\n",
    "    df = df.rename(columns={\"discountPercentage\": \"discount_percentage\"})\n",
    "\n",
    "    # Expand 'reviews' into multiple rows\n",
    "    df = df.explode(\"reviews\", ignore_index=True)\n",
    "\n",
    "    # Extract rating and reviewer name\n",
    "    df[\"reviewer_rating\"] = df[\"reviews\"].apply(lambda x: x.get(\"rating\") if isinstance(x, dict) else None)\n",
    "    df[\"reviewer_name\"] = df[\"reviews\"].apply(lambda x: x.get(\"reviewerName\") if isinstance(x, dict) else None)\n",
    "\n",
    "    df = df.drop(columns=[\"reviews\"])\n",
    "    \n",
    "    # Drop invalid/missing or duplicated data \n",
    "    df = df.dropna(subset=[\"id\", \"title\"], how=\"all\")\n",
    "    df = df.dropna(subset=[\"price\", \"reviewer_rating\", \"reviewer_name\"])\n",
    "    df = df.drop_duplicates(subset=[\"id\", \"reviewer_name\"])\n",
    "\n",
    "    df = df.loc[(df[\"price\"] > 0) & (df[\"discount_percentage\"].between(0, 100))]\n",
    "\n",
    "    # Calculate discounted price\n",
    "    df[\"price_with_discount\"] = (df[\"price\"] * (1 - df[\"discount_percentage\"] / 100)).round(2)\n",
    "\n",
    "    # Ensure correct data types\n",
    "    df = df.astype({\n",
    "        \"price\": float,\n",
    "        \"discount_percentage\": float,\n",
    "        \"rating\": float,\n",
    "        \"reviewer_rating\": int,\n",
    "        \"reviewer_name\": str,\n",
    "        \"price_with_discount\": float\n",
    "    })\n",
    "\n",
    "    # Reorder and standardize columns\n",
    "    df = df[[\"id\", \"title\", \"category\", \"price\", \"discount_percentage\",\n",
    "             \"brand\", \"reviewer_rating\", \"reviewer_name\", \"price_with_discount\"]] \n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97e6f9",
   "metadata": {},
   "source": [
    "### 3.3 Load Function\n",
    "* Loads DataFrame into a staging table temporarily.\n",
    "* Inserts data from staging into the main table, updating existing rows on conflict (id + reviewer_name).\n",
    "* Automatically sets `created_at` and `updated_at` timestamps.\n",
    "* Uses transactions to ensure all-or-nothing data integrity.\n",
    "* Optionally drops the staging table after load (default: True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03f9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(df, sql_connection, table_name, drop_staging=True):\n",
    "    schema = \"etl_schema\"\n",
    "    staging_table = f\"{table_name}_staging\"\n",
    "\n",
    "    #df to string for dynamic sql\n",
    "    data_columns = \", \".join(df.columns)\n",
    "    \n",
    "    with sql_connection.begin() as conn:\n",
    "        # Load to staging\n",
    "        df.to_sql(staging_table, conn, if_exists=\"replace\", index=False, schema=schema)\n",
    "        logging.info(f\"Loaded {len(df)} records to {schema}.{staging_table}\")\n",
    "        \n",
    "        # Merge staging into main table with conflict handling\n",
    "        merge_query = text(f\"\"\"\n",
    "            INSERT INTO {schema}.{table_name} (\n",
    "                {data_columns}, created_at, updated_at\n",
    "            )\n",
    "            SELECT {data_columns}, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP\n",
    "            FROM {schema}.{staging_table}\n",
    "            ON CONFLICT (id, reviewer_name)\n",
    "            DO UPDATE SET reviewer_name = EXCLUDED.reviewer_name,\n",
    "              reviewer_rating = EXCLUDED.reviewer_rating,\n",
    "              price = EXCLUDED.price,\n",
    "              discount_percentage = EXCLUDED.discount_percentage,\n",
    "              price_with_discount = EXCLUDED.price_with_discount;\n",
    "        \"\"\")\n",
    "        conn.execute(merge_query)\n",
    "        logging.info(f\"Inserted data to {schema}.{table_name}\")\n",
    "\n",
    "        # Drop staging\n",
    "        if drop_staging:\n",
    "            conn.execute(text(f\"DROP TABLE IF EXISTS {schema}.{staging_table}\"))\n",
    "            logging.info(\"Dropped staging table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e924bd",
   "metadata": {},
   "source": [
    "### 4. Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe033a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Connection\n",
    "engine = create_engine(\n",
    "    'postgresql://etl_user:sifre@localhost:5432/etl_pipeline'  \n",
    ")\n",
    "\n",
    "try:\n",
    "    # Extract\n",
    "    df = extract(\"https://dummyjson.com/products\")\n",
    "    logging.info(f\"Extract completed: {len(df)} raw records\")\n",
    "    \n",
    "    # Transform\n",
    "    df_t = transform(df)\n",
    "    logging.info(f\"Transform completed: {len(df_t)} clean records\")\n",
    "    \n",
    "    # Load\n",
    "    load(df_t, engine, table_name=\"products\", drop_staging=True)\n",
    "    logging.info(\"ETL Pipeline completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.critical(f\"Pipeline failed: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    engine.dispose()\n",
    "    logging.info(\"Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
