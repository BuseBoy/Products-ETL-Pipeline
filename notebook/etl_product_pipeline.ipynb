{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0448f2",
   "metadata": {},
   "source": [
    "## ETL Product Data Pipeline\n",
    "In this notebook, we will build a robust ETL pipeline to process product data from an API. The pipeline will:\n",
    "* Extract product data from multiple API endpoints or clients.\n",
    "* Transform the data to standardize column names, handle nested or missing values, and calculate additional metrics like discounted price.\n",
    "* Load the cleaned data into a PostgreSQL database using a staging-to-main table pattern.\n",
    "* Log all steps and errors for traceability and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a4d66",
   "metadata": {},
   "source": [
    "### 1. Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653f4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7400a",
   "metadata": {},
   "source": [
    "### 2. Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5339cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename=\"etl_product_pipeline.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    \n",
    ")\n",
    "logging.info(\"ETL pipeline started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb76d9",
   "metadata": {},
   "source": [
    "## 3. Defining Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c631bd8",
   "metadata": {},
   "source": [
    "### 3.1 Extract Function\n",
    "* Fetches paginated data from REST API endpoints with configurable page limits.\n",
    "* Extracts specified key from JSON response; falls back to full response if key missing.\n",
    "* Stops early when total record count is reached to avoid unnecessary requests.\n",
    "* Skips empty pages and continues fetching remaining data.\n",
    "* Logs page progress, errors, and completion status for monitoring.\n",
    "* Returns combined data from all pages as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20719f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(base_url, key=\"products\", limit=30, max_pages=20):\n",
    "    all_data = []\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        for page in range(max_pages):\n",
    "            url = f\"{base_url}?limit={limit}&skip={page * limit}\"\n",
    "            try:\n",
    "                response = session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                # Extract relevant data\n",
    "                page_data = data.get(key, data)\n",
    "                if not page_data:\n",
    "                    logging.debug(f\"Empty page {page}, continuing\")\n",
    "                    continue\n",
    "                \n",
    "                all_data.extend(page_data)\n",
    "                logging.info(f\"Page {page}: {len(page_data)} records\")\n",
    "                \n",
    "                # Stop if we've fetched all records\n",
    "                if (total := data.get(\"total\")) and len(all_data) >= total:\n",
    "                    logging.info(f\"All {total} records fetched\")\n",
    "                    break\n",
    "                    \n",
    "            except requests.RequestException as e:\n",
    "                logging.error(f\"Error on page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    logging.info(f\"Extract completed: {len(df)} records fetched\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd1deb",
   "metadata": {},
   "source": [
    "### 3.2 Transform Function\n",
    "* Explodes nested reviews into separate rows for granular analysis.\n",
    "* Extracts review ratings from nested dictionaries and removes original review column.\n",
    "* Drops rows with missing critical fields (id, title, price, review_rating).\n",
    "* Validates and filters data: removes invalid prices and out-of-range discounts.\n",
    "* Calculates discounted price based on original price and discount percentage.\n",
    "* Converts all columns to appropriate data types (float, int).\n",
    "* Selects and reorders relevant columns, normalizes names to lowercase.\n",
    "* Returns clean, analysis-ready DataFrame with consistent structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541cb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "   \n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.explode(\"reviews\", ignore_index=True)\n",
    "    df[\"review_rating\"] = df[\"reviews\"].apply(lambda x: x[\"rating\"] if isinstance(x, dict) else None)\n",
    "    df = df.drop(columns=[\"reviews\"])\n",
    "    \n",
    "    df = df.dropna(subset=[\"id\", \"title\"], how=\"all\")\n",
    "    df = df.dropna(subset=[\"price\",\"review_rating\"]) \n",
    "\n",
    "    df = df[df['price'] > 0]\n",
    "    df = df[(df['discountPercentage'] >= 0) & (df['discountPercentage'] <= 100)]        \n",
    "\n",
    "    df[\"price_with_discount\"] = (df[\"price\"] * (1 - df[\"discountPercentage\"] / 100)).round(2)\n",
    "\n",
    "    df[\"price\"] = df[\"price\"].astype(float)\n",
    "    df[\"discountPercentage\"] = df[\"discountPercentage\"].astype(float)\n",
    "    df[\"rating\"] = df[\"rating\"].astype(float)\n",
    "    df[\"review_rating\"] = df[\"review_rating\"].astype(int)\n",
    "    df[\"price_with_discount\"] = df[\"price_with_discount\"].astype(float)\n",
    "\n",
    "    df = df[[\"id\", \"title\", \"category\", \"price\", \"discountPercentage\",\n",
    "          \"rating\", \"brand\", \"review_rating\", \"price_with_discount\"]]\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97e6f9",
   "metadata": {},
   "source": [
    "### 3.3 Load Function\n",
    "* Creates a staging table and loads DataFrame into it temporarily.\n",
    "* Inserts all records from staging into the main table with timestamps.\n",
    "* Adds `created_at` and `updated_at` columns automatically during insert.\n",
    "* Uses SQL transactions to ensure data integrity (all-or-nothing).\n",
    "* Optionally drops the staging table after successful load (default: True).\n",
    "* Logs each step: staging load, data insert, and staging table cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03f9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(df, sql_connection, table_name, drop_staging=True):\n",
    "    from sqlalchemy import text\n",
    "    schema = 'etl_schema'\n",
    "    staging_table = f\"{table_name}_staging\"\n",
    "    \n",
    "    with sql_connection.begin() as conn:\n",
    "        # Load to staging\n",
    "        df.to_sql(staging_table, conn, if_exists='replace', index=False, schema=schema)\n",
    "        logging.info(f\"Loaded {len(df)} records to {schema}.{staging_table}\")\n",
    "        \n",
    "        # Insert all records from staging\n",
    "        merge_query = text(f\"\"\"\n",
    "            INSERT INTO {schema}.{table_name} (\n",
    "                id, title, category, price, discountPercentage,\n",
    "                rating, brand, review_rating, price_with_discount,\n",
    "                created_at, updated_at\n",
    "            )\n",
    "            SELECT id, title, category, price, discountPercentage, rating, brand,\n",
    "                review_rating, price_with_discount,\n",
    "                CURRENT_TIMESTAMP, CURRENT_TIMESTAMP\n",
    "            FROM {schema}.{staging_table};\n",
    "        \"\"\")\n",
    "        conn.execute(merge_query)\n",
    "        logging.info(f\"Inserted data to {schema}.{table_name}\")\n",
    "        \n",
    "        # Drop staging\n",
    "        if drop_staging:\n",
    "            conn.execute(text(f\"DROP TABLE IF EXISTS {schema}.{staging_table}\"))\n",
    "            logging.info(f\"Dropped staging table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e924bd",
   "metadata": {},
   "source": [
    "### 4. Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe033a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Connection\n",
    "engine = create_engine(\n",
    "    'postgresql://etl_user:sifre@localhost:5432/etl_pipeline'  # .env oluÅŸturulabilirdi.\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Extract\n",
    "    df = extract(\"https://dummyjson.com/products\", limit=30)\n",
    "    logging.info(f\"Extract completed: {len(df)} raw records\")\n",
    "    \n",
    "    # Transform\n",
    "    df_t = transform(df)\n",
    "    logging.info(f\"Transform completed: {len(df_t)} clean records\")\n",
    "    \n",
    "    # Load\n",
    "    load(df_t, engine, table_name=\"products\", drop_staging=True)\n",
    "    logging.info(\"ETL Pipeline completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.critical(f\"Pipeline failed: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    engine.dispose()\n",
    "    logging.info(\"Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
